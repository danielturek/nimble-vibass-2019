---
title: "Customizing an MCMC: extended example"
subtitle: "Valencia International Bayesian Analysis Summer School Workshop"
output: html_document
---

```{r chunksetup, include=FALSE} 
# include any code here you don't want to show up in the document,
# e.g. package and dataset loading
library(methods)  # otherwise new() not being found - weird
library(nimble)
library(coda, warn.conflicts = FALSE)
read_chunk('chunks_litters.R')
```

# The litters model

Here's the graph of the litters model.

<center><img src="littersDAG.jpg"></center>

Here we set up the litters model.

```{r, litters-code}
```
```{r, litters-model}
```
```{r, litters-compile}
```
```{r, makePlot, echo=FALSE}
```

# Blocking parameters

Often a key factor that reduces MCMC performance is dependence between parameters that limits the ability of univariate samplers to move very far. A standard strategy is to sample correlated parameters in blocks. Unlike many other MCMC engines, NIMBLE makes it easy for users to choose what parameters to sample in blocks.

We'll try that here for ```a``` and ```b```.

```{r, prep, echo=FALSE}
# so attendees can run code below this without using code from other modules
if(FALSE)
   if(!exists('littersModel') || !exists('cLittersModels')) source('chunks_litters.R')
```                   

```{r customize-mcmc2}
niter <- 5000
nburn <- 1000

littersConf <- configureMCMC(littersModel, monitors = c('a', 'b', 'p'))
hypers <- littersModel$getNodeNames(topOnly = TRUE)
print(hypers)
for(h in hypers) {
      littersConf$removeSamplers(h)
}
littersConf$addSampler(target = c('a[1]','b[1]'), type = 'RW_block', 
                              control = list(adaptInterval = 100))
littersConf$addSampler(target = c('a[2]','b[2]'), type = 'RW_block', 
                              control = list(adaptInterval = 100))

littersMCMC <- buildMCMC(littersConf)
cLittersMCMC <- compileNimble(littersMCMC, project = littersModel, resetFunctions = TRUE)

set.seed(1)
samplesBlock <- runMCMC(cLittersMCMC, niter = niter, nburnin = nburn,
             inits = littersInits, nchains = 1, samplesAsCodaMCMC = TRUE)
```

```{r output-block, fig.height=6, fig.width=12, fig.cap=''}
effectiveSize(samplesBlock)

library(basicMCMCplots)
basicMCMCplots::chainsPlot(samplesBlock,
                           var = c("a", "b"),
                           cex = 1.6)
```

The block sampler seems to help some, but hopefully we can do better. Often block sampling gives bigger improvements.


# Blocking the random effects too

But perhaps we should have blocked the hyperparameters with their dependent random effects. This is how one could do that, though ```a```, ```b```, and ```p``` are on very different scales, which may cause problems, particularly at the start of an adaptive sampler. As we see in the trace plots, this strategy is not working at all.

```{r, effects-block, fig.height=6, fig.width=12, fig.cap=''}
littersConf$removeSamplers(c('a', 'b', 'p'))
group1nodes <- littersModel$getDependencies(c('a[1]', 'b[1]'), stochOnly = TRUE)
group2nodes <- littersModel$getDependencies(c('a[2]', 'b[2]'), stochOnly = TRUE)
group1nodes
propCov <- diag(c(.5, .5, rep(.01, 16)))
littersConf$addSampler(group1nodes, 'RW_block', control =
                       list(adaptInterval = 100, propCov = propCov))
littersConf$addSampler(group2nodes, 'RW_block', control =
                       list(adaptInterval = 100, propCov = propCov))

littersMCMC <- buildMCMC(littersConf)
cLittersMCMC <- compileNimble(littersMCMC, project = littersModel, resetFunctions = TRUE)

set.seed(1)
samplesSuperblock <- runMCMC(cLittersMCMC, niter = niter, nburnin = nburn,
                  inits = littersInits, nchains = 1, samplesAsCodaMCMC = TRUE)
```

```{r output-superblock, fig.height=6, fig.width=12, fig.cap=''}
effectiveSize(samplesSuperblock)

basicMCMCplots::chainsPlot(samplesSuperblock,
                           var = c("a", "b"),
                           cex = 1.6)
```

I suspect that more time for adaptation is needed (and perhaps better initialization of the proposal covariance).

# Implicitly integrating over the random effects: cross-level sampler

Note that in this model, one could analytically integrate over the random effects (necessarily so since we have conjugacy). In NIMBLE this is pretty easy to do using user-defined distributions (next module), though it requires some technical knowledge of working with distributions.

An easier alternative to analytically integrating over the random effects is to use a computational trick that mathematically achieves the same result.

That is NIMBLE's *cross-level sampler*. Here's what it does:

  - do a blocked Metropolis random walk on one or more hyperparameters and
  - then a conjugate update of the dependent nodes conditional on the proposed hyperparameters,
  - accepting/rejecting everything together

Comments: 
  - this amounts to a joint update of the hyperparameters and their dependent nodes
  - equivalent to analytically integrating over the dependent nodes
  - this is the *one-block* sampler in <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9469.00308" target="_blank" style="color: blue">Knorr-Held and Rue (2002)</a>

# Applying the cross-level sampler


```{r, cross-level, fig.height=6, fig.width=12, fig.cap=''}
littersConf$removeSamplers(c('a', 'b', 'p'))
littersConf$addSampler(c('a[1]', 'b[1]'), 'crossLevel')
littersConf$addSampler(c('a[2]', 'b[2]'), 'crossLevel')

littersMCMC <- buildMCMC(littersConf)
cLittersMCMC <- compileNimble(littersMCMC, project = littersModel, resetFunctions = TRUE)

set.seed(1)
samplesCross <- runMCMC(cLittersMCMC, niter = niter, nburnin = nburn,
             inits = littersInits, nchains = 1, samplesAsCodaMCMC = TRUE)
```

# Cross-level sampler results

```{r output-cross-level, fig.height=6, fig.width=12, fig.cap=''}
effectiveSize(samplesCross)

basicMCMCplots::chainsPlot(samplesCross,
                           var = c("a", "b"),
                           cex = 1.6)
```

Much better, though we'd still want to look into the lack of movement for `a[1], b[1]` in the initial non-burnin samples -- this could probably be improved with better initialization of the top-level block sampler. 

