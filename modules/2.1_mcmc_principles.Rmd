---
title: "MCMC principles"
subtitle: "Valencia International Bayesian Analysis Summer School Workshop"
output: html_document
---

# MCMC

- A family of algorithms for iteratively sampling from a posterior distribution
- Various kinds of 'samplers' exist for sampling from one or more unknown 'parameters'
    - Metropolis-Hastings
    - slice sampling
    - Gibbs (i.e., conjugate / full conditional) sampling
    - Hamiltonian Monte Carlo
    - many, many others
- Blocking - various options for how to group parameters when sampling
    - cycle through each univariate parameter (e.g., Metropolis within Gibbs)
    - sample all parameters in a block
    - cycle through user-defined blocks of samplers (blocks can overlap)
   
Generally, as long as at least one sampler is applied to each unknown, the algorithm will be valid.

# Blocking

- univariate sampling is straightforward but struggles with strong dependence
    - with limited dependence, univariate samplers can often explore more quickly
    - with strong dependence, movements in one dimension strongly constrained by current value(s) in other dimensions
    - when parameters are part of a multivariate distribution, univariate sampling generally involves extra computation
- block samplers can account for dependence but don't explore as quickly with limited dependence
    - conjugate samplers for blocks work really well
    - often hard to develop good non-conjugate samplers under dependence
    - Hamiltonian Monte Carlo can work very well in some cases but each sample is computationally expensive and gradients are required


NIMBLE allows:

  - arbitrary blocking and
  - vectorized model declarations that are efficient with block sampling

# Marginalization

- Marginalized models:
    - Example: litters model
    - Advantages
        - Reduced dimension reduces computation
        - Removing 'levels' reduces dependence across levels; moves parameters 'closer' to data
        - Removing correlated parameters removes need for good multivariate proposals
    - Disadvantages
        - Requires some mathematical/probability understanding
        - Can induce complicated dependence

<!--
# Marginalization and data augmentation

- Data augmentation:
    - Examples:
        - Chib and Albert probit regression 'trick'
        - t distribution as mixture of normal distributions (see Gelman et al. Bayesian Data Analysis for sampling tips)
    - Advantages
        - May allow for conjugate or other useful samplers; particular helpful for multivariate blocks
    - Disadvantages
        - More computation; sometimes greatly increases dimensionality
        - More 'levels' increases dependence across levels; moves parameters 'further' from data
-->

# Some strategies for improving MCMC (in NIMBLE)

  - Identify and focus on the worst-mixing parameters
  - Blocking
  - Try various samplers
  - Think like the graph:
      - Write vectorized deterministic and stochastic BUGS declarations for parameters that will be sampled in a block
      - Avoid vectorized BUGS declarations for parameters that will be sampled individually
  - Defer posterior-predictive sampling until after the main MCMC for parameter estimation.

(Note that vectorization in (compiled) nimbleFunctions often has little effect because loops are fast in compiled code.)
