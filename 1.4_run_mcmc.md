---
title: "Running an MCMC"
subtitle: "Valencia International Bayesian Analysis Summer School Workshop"
output: html_document
---



# A very basic MCMC

The steps of running an MCMC are as follows:

 1. configure the MCMC (via `configureMCMC()`)
 2. build the MCMC (via `buildMCMC()`)
 3. create a compiled version of the MCMC (via `compileNimble()`)
 4. run the MCMC (via `runMCMC()`)
 5. assess and use the MCMC samples (e.g., using CODA tools)

Note that we can combine steps 1-4 by using `nimbleMCMC()` (which in fact does not even require you to create the model), but if we want to modify the default MCMC configuration of samplers then we need to separate the steps. We'll see `nimbleMCMC()` a bit later.

# Build the model

We first need to build the model as we did in the previous module.


```r
library(nimble)
littersCode <- nimbleCode({
  for (i in 1:G) {
     for (j in 1:N) {
        # likelihood (data model)
        r[i,j] ~ dbin(p[i,j], n[i,j])
        # latent process (random effects)
        p[i,j] ~ dbeta(a[i], b[i]) 
     }
     # prior for hyperparameters
     a[i] ~ dgamma(1, .001)
     b[i] ~ dgamma(1, .001)
   }
})
```


```r
## data and constants as R objects
G <- 2
N <- 16
n <- matrix(c(13, 12, 12, 11, 9, 10, 
              9, 9, 8, 11, 8, 10, 13, 10, 12, 9, 10, 9, 10, 5, 9, 9, 13, 
              7, 5, 10, 7, 6, 10, 10, 10, 7), nrow = 2)
r <- matrix(c(13, 12, 12, 11, 9, 10, 9, 9, 8, 10, 8, 9, 
     12, 9, 11, 8, 9, 8, 9, 4, 8, 7, 11, 4, 4, 5, 5, 3, 7, 3, 7, 0), 
     nrow = 2)
              
littersConsts <- list(G = G, N = N, n = n)
littersData <- list(r = r)
littersInits <- list( a = c(2, 2), b=c(2, 2) )

## create the NIMBLE model object
littersModel <- nimbleModel(littersCode, 
          data = littersData, constants = littersConsts, inits = littersInits)
```

```
## defining model...
```

```
## building model...
```

```
## setting data and initial values...
```

```
## running calculate on model (any error reports that follow may simply reflect missing values in model variables) ... 
## checking model sizes and dimensions... This model is not fully initialized. This is not an error. To see which variables are not initialized, use model$initializeInfo(). For more information on model initialization, see help(modelInitialization).
## model building finished.
```


```r
cLittersModel <- compileNimble(littersModel)
```

```
## compiling... this may take a minute. Use 'showCompilerOutput = TRUE' to see C++ compilation details.
```

```
## compilation finished.
```

# Configuring a basic MCMC

Setting up and running an MCMC in NIMBLE in this way takes a few more steps than in BUGS or JAGS, but with the benefit of giving the user much more control of how the MCMC operates.

First we *configure* the MCMC, which means setting up the samplers to be used for each node or group of nodes. NIMBLE provides a default configuration, but we'll see shortly how you can modify that. 




```r
littersConf <- configureMCMC(littersModel, print = TRUE)
```

```
## [1]  RW sampler: a[1]
## [2]  RW sampler: a[2]
## [3]  RW sampler: b[1]
## [4]  RW sampler: b[2]
## [5]  conjugate_dbeta_dbin sampler: p[1, 1]
## [6]  conjugate_dbeta_dbin sampler: p[1, 2]
## [7]  conjugate_dbeta_dbin sampler: p[1, 3]
## [8]  conjugate_dbeta_dbin sampler: p[1, 4]
## [9]  conjugate_dbeta_dbin sampler: p[1, 5]
## [10] conjugate_dbeta_dbin sampler: p[1, 6]
## [11] conjugate_dbeta_dbin sampler: p[1, 7]
## [12] conjugate_dbeta_dbin sampler: p[1, 8]
## [13] conjugate_dbeta_dbin sampler: p[1, 9]
## [14] conjugate_dbeta_dbin sampler: p[1, 10]
## [15] conjugate_dbeta_dbin sampler: p[1, 11]
## [16] conjugate_dbeta_dbin sampler: p[1, 12]
## [17] conjugate_dbeta_dbin sampler: p[1, 13]
## [18] conjugate_dbeta_dbin sampler: p[1, 14]
## [19] conjugate_dbeta_dbin sampler: p[1, 15]
## [20] conjugate_dbeta_dbin sampler: p[1, 16]
## [21] conjugate_dbeta_dbin sampler: p[2, 1]
## [22] conjugate_dbeta_dbin sampler: p[2, 2]
## [23] conjugate_dbeta_dbin sampler: p[2, 3]
## [24] conjugate_dbeta_dbin sampler: p[2, 4]
## [25] conjugate_dbeta_dbin sampler: p[2, 5]
## [26] conjugate_dbeta_dbin sampler: p[2, 6]
## [27] conjugate_dbeta_dbin sampler: p[2, 7]
## [28] conjugate_dbeta_dbin sampler: p[2, 8]
## [29] conjugate_dbeta_dbin sampler: p[2, 9]
## [30] conjugate_dbeta_dbin sampler: p[2, 10]
## [31] conjugate_dbeta_dbin sampler: p[2, 11]
## [32] conjugate_dbeta_dbin sampler: p[2, 12]
## [33] conjugate_dbeta_dbin sampler: p[2, 13]
## [34] conjugate_dbeta_dbin sampler: p[2, 14]
## [35] conjugate_dbeta_dbin sampler: p[2, 15]
## [36] conjugate_dbeta_dbin sampler: p[2, 16]
```
You also specify the nodes for which you'd like to get the MCMC samples as output. (NIMBLE defaults to only monitoring the "top-level" nodes, i.e, hyperparameters with no stochastic parents.


```r
littersConf$addMonitors(c('a', 'b', 'p'))
```

```
## thin = 1: a, b, p
```

# Building the MCMC algorithm for the model 

Next we'll build the MCMC algorithm for the model under the default configuration. And we'll create a compiled (i.e., C++) version of the MCMC that is equivalent in functionality but will run much faster.


```r
littersMCMC <- buildMCMC(littersConf)
cLittersMCMC <- compileNimble(littersMCMC, project = littersModel)
```

```
## compiling... this may take a minute. Use 'showCompilerOutput = TRUE' to see C++ compilation details.
```

```
## compilation finished.
```

(The *project* argument helps us manage all the C++ that is generated for a given analysis. In general the project can be referenced using the name of the original (uncompiled) model.)

# Running the MCMC

Now let's run the MCMC. We don't recommend running the R version of the MCMC for very many iterations - it's really slow - in part because iterating in R is slow and in part because iterating with a model in NIMBLE requires even more overhead. 


```r
niter <- 1000
nburn <- 100
set.seed(1)
inits <- function() {
      a <- runif(G, 1, 20)
      b <- runif(G, 1, 20)
      p <- rbind(rbeta(N, a[1], b[1]), rbeta(N, a[2], b[2]))
      return(list(a = a, b = b, p = p))
}             
print(system.time(samples <- runMCMC(cLittersMCMC, niter = niter, nburnin = nburn,
                          inits = inits, nchains = 3, samplesAsCodaMCMC = TRUE)))
```

```
## running chain 1...
```

```
## |-------------|-------------|-------------|-------------|
## |-------------------------------------------------------|
```

```
## running chain 2...
```

```
## |-------------|-------------|-------------|-------------|
## |-------------------------------------------------------|
```

```
## running chain 3...
```

```
## |-------------|-------------|-------------|-------------|
## |-------------------------------------------------------|
##    user  system elapsed 
##   0.160   0.002   0.162
```

# Working with MCMC output

The R and C MCMC samples are the same, so you can use the R MCMC for debugging. It's possible to step through the code line by line using R's debugging capabilities (not shown).

Now let's look at the MCMC performance from one of the chains.


```r
samples1 <- samples[[1]]
par(mfrow = c(2, 2), mai = c(.6, .5, .4, .1), mgp = c(1.8, 0.7, 0))
ts.plot(samples1[ , 'a[1]'], xlab = 'iteration',
     ylab = expression(a[1]), main = expression(a[1]))
ts.plot(samples1[ , 'b[1]'], xlab = 'iteration',
     ylab = expression(b[1]), main = expression(b[1]))
ts.plot(samples1[ , 'a[2]'], xlab = 'iteration',
     ylab = expression(a[2]), main = expression(a[2]))
ts.plot(samples1[ , 'b[2]'], xlab = 'iteration',
     ylab = expression(b[2]), main = expression(b[2]))
```

![](figure/output-mcmc-1.png)

Not good. We'll explore different sampling strategies that fix the problems in the next module.

# Using CODA

NIMBLE does not provide any MCMC diagnostics. (At least not yet; there's no reason one couldn't write code for various diagnostics using the NIMBLE system.)  But one can easily use CODA or other R packages with the MCMC output from a NIMBLE MCMC.


```r
library(coda, warn.conflicts = FALSE)
crosscorr(samples1[ , c('a[1]', 'b[1]', 'a[2]', 'b[2]')])
```

```
##            a[1]          b[1]          a[2]       b[2]
## a[1] 1.00000000  0.8984191081  0.0421801087 0.15348784
## b[1] 0.89841911  1.0000000000 -0.0005115732 0.09739841
## a[2] 0.04218011 -0.0005115732  1.0000000000 0.76531858
## b[2] 0.15348784  0.0973984082  0.7653185840 1.00000000
```

```r
effectiveSize(samples1)  ## ESS
```

```
##       a[1]       a[2]       b[1]       b[2]    p[1, 1]    p[2, 1] 
##   1.308928  26.160901   1.590261  22.939173  21.874297 591.262047 
##    p[1, 2]    p[2, 2]    p[1, 3]    p[2, 3]    p[1, 4]    p[2, 4] 
##  20.469789 419.709707  20.709110 423.115846  18.327761 377.784384 
##    p[1, 5]    p[2, 5]    p[1, 6]    p[2, 6]    p[1, 7]    p[2, 7] 
##  42.908928 900.000000  45.122292 488.602386  41.497495 900.000000 
##    p[1, 8]    p[2, 8]    p[1, 9]    p[2, 9]   p[1, 10]   p[2, 10] 
##  25.469092 900.000000  35.765405 617.858782  35.685197 900.000000 
##   p[1, 11]   p[2, 11]   p[1, 12]   p[2, 12]   p[1, 13]   p[2, 13] 
##  35.432174 900.000000  23.656455 796.376290  35.811872 350.405032 
##   p[1, 14]   p[2, 14]   p[1, 15]   p[2, 15]   p[1, 16]   p[2, 16] 
##  14.138105 505.561999   9.338020 298.215924   9.240530  96.404600
```

To apply the commonly used Gelman-Rubin potential scale reduction factor diagnostic, we'll need the multiple chains.

Considerations: you'll want to think about how to set up the over-dispersed starting points and the number of iterations to use for burn-in.

# Assessing MCMC performance from multiple chains


```r
par(mfrow = c(1,1))
gelman.diag(samples)
```

```
## Potential scale reduction factors:
## 
##          Point est. Upper C.I.
## a[1]           2.04       4.55
## a[2]           1.15       1.47
## b[1]           1.90       3.68
## b[2]           1.18       1.54
## p[1, 1]        1.04       1.12
## p[2, 1]        1.03       1.10
## p[1, 2]        1.03       1.09
## p[2, 2]        1.02       1.06
## p[1, 3]        1.02       1.07
## p[2, 3]        1.02       1.07
## p[1, 4]        1.03       1.10
## p[2, 4]        1.02       1.06
## p[1, 5]        1.03       1.07
## p[2, 5]        1.01       1.04
## p[1, 6]        1.03       1.07
## p[2, 6]        1.01       1.03
## p[1, 7]        1.04       1.10
## p[2, 7]        1.00       1.01
## p[1, 8]        1.04       1.10
## p[2, 8]        1.00       1.00
## p[1, 9]        1.04       1.10
## p[2, 9]        1.01       1.03
## p[1, 10]       1.03       1.09
## p[2, 10]       1.00       1.00
## p[1, 11]       1.04       1.11
## p[2, 11]       1.00       1.01
## p[1, 12]       1.07       1.21
## p[2, 12]       1.01       1.04
## p[1, 13]       1.08       1.21
## p[2, 13]       1.01       1.04
## p[1, 14]       1.10       1.30
## p[2, 14]       1.02       1.08
## p[1, 15]       1.19       1.59
## p[2, 15]       1.02       1.09
## p[1, 16]       1.15       1.45
## p[2, 16]       1.05       1.17
## 
## Multivariate psrf
## 
## 1.66
```

```r
## and here's a graphical representation of the information
par(mfrow = c(1, 2))
ts.plot(samples[[1]][ , 'a[1]'], xlab = 'iteration',
     ylab = expression(a[1]), main = expression(a[1]))
sq <- seq_along(samples[[1]][ , 'a[1]'])
for(i in 2:3)
      lines(sq, samples[[i]][ , 'a[1]'], col = i)
ts.plot(samples[[1]][ , 'b[1]'], xlab = 'iteration',
     ylab = expression(b[1]), main = expression(a[1]))
sq <- seq_along(samples[[1]][ , 'b[1]'])
for(i in 2:3)
      lines(sq, samples[[i]][ , 'b[1]'], col = i)
```

![](figure/gelman-rubin-1.png)

# Other ways to run MCMC

- To quickly combine all steps, use `nimbleMCMC()`.
- To take full control, use `cLittersMCMC$run(niter)`
- To compare multiple MCMC configurations, we'll learn about the `compareMCMCs` package.

# Other MCMC tools in NIMBLE

  - WAIC for model comparison
  - cross-validation
  - (coming soon) calibrated posterior predictive p-values
