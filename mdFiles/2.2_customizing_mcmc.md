---
title: "Customizing an MCMC"
subtitle: "Valencia International Bayesian Analysis Summer School Workshop"
output: html_document
---



# The litters model

Here's the graph of the litters model.

<center><img src="littersDAG.jpg"></center>

Here we set up the litters model.


```r
library(nimble)
littersCode <- nimbleCode({
  for (i in 1:G) {
     for (j in 1:N) {
        # likelihood (data model)
        r[i,j] ~ dbin(p[i,j], n[i,j])
        # latent process (random effects)
        p[i,j] ~ dbeta(a[i], b[i]) 
     }
     # prior for hyperparameters
     a[i] ~ dgamma(1, .001)
     b[i] ~ dgamma(1, .001)
   }
})
```

```r
## data and constants as R objects
G <- 2
N <- 16
n <- matrix(c(13, 12, 12, 11, 9, 10, 
              9, 9, 8, 11, 8, 10, 13, 10, 12, 9, 10, 9, 10, 5, 9, 9, 13, 
              7, 5, 10, 7, 6, 10, 10, 10, 7), nrow = 2)
r <- matrix(c(13, 12, 12, 11, 9, 10, 9, 9, 8, 10, 8, 9, 
     12, 9, 11, 8, 9, 8, 9, 4, 8, 7, 11, 4, 4, 5, 5, 3, 7, 3, 7, 0), 
     nrow = 2)
              
littersConsts <- list(G = G, N = N, n = n)
littersData <- list(r = r)
littersInits <- list( a = c(2, 2), b=c(2, 2) )

## create the NIMBLE model object
littersModel <- nimbleModel(littersCode, 
          data = littersData, constants = littersConsts, inits = littersInits)
```

```
## defining model...
```

```
## building model...
```

```
## setting data and initial values...
```

```
## running calculate on model (any error reports that follow may simply reflect missing values in model variables) ... 
## checking model sizes and dimensions... This model is not fully initialized. This is not an error. To see which variables are not initialized, use model$initializeInfo(). For more information on model initialization, see help(modelInitialization).
## model building finished.
```

```r
cLittersModel <- compileNimble(littersModel)
```

```
## compiling... this may take a minute. Use 'showCompilerOutput = TRUE' to see C++ compilation details.
```

```
## compilation finished.
```


# NIMBLE's default MCMC

Here are the results from running NIMBLE's default MCMC:




```r
littersConf <- configureMCMC(littersModel, monitors = c('a', 'b', 'p'))
littersMCMC <- buildMCMC(littersConf)
cLittersMCMC <- compileNimble(littersMCMC, project = littersModel)
```

```
## compiling... this may take a minute. Use 'showCompilerOutput = TRUE' to see C++ compilation details.
```

```
## compilation finished.
```

```r
niter <- 5000
nburn <- 1000
set.seed(1)
samples <- runMCMC(cLittersMCMC, niter = niter, nburnin = nburn,
        inits = littersInits, nchains = 1, samplesAsCodaMCMC = TRUE)
```

```
## running chain 1...
```

```
## |-------------|-------------|-------------|-------------|
## |-------------------------------------------------------|
```


```r
library(basicMCMCplots)
basicMCMCplots::chainsPlot(samples,
                           var = c("a", "b"),
                           cex = 1.6)
```

![](figure/plot-samples-litters-1.png)

# Customizing samplers: examining the defaults

One of NIMBLE's most important features is that users can easily modify the MCMC algorithm used for their model. The easiest thing to do is to start with NIMBLE's default MCMC and then make modifications. 


```r
littersConf$printSamplers()
```

```
## [1]  RW sampler: a[1]
## [2]  RW sampler: a[2]
## [3]  RW sampler: b[1]
## [4]  RW sampler: b[2]
## [5]  conjugate_dbeta_dbin sampler: p[1, 1]
## [6]  conjugate_dbeta_dbin sampler: p[1, 2]
## [7]  conjugate_dbeta_dbin sampler: p[1, 3]
## [8]  conjugate_dbeta_dbin sampler: p[1, 4]
## [9]  conjugate_dbeta_dbin sampler: p[1, 5]
## [10] conjugate_dbeta_dbin sampler: p[1, 6]
## [11] conjugate_dbeta_dbin sampler: p[1, 7]
## [12] conjugate_dbeta_dbin sampler: p[1, 8]
## [13] conjugate_dbeta_dbin sampler: p[1, 9]
## [14] conjugate_dbeta_dbin sampler: p[1, 10]
## [15] conjugate_dbeta_dbin sampler: p[1, 11]
## [16] conjugate_dbeta_dbin sampler: p[1, 12]
## [17] conjugate_dbeta_dbin sampler: p[1, 13]
## [18] conjugate_dbeta_dbin sampler: p[1, 14]
## [19] conjugate_dbeta_dbin sampler: p[1, 15]
## [20] conjugate_dbeta_dbin sampler: p[1, 16]
## [21] conjugate_dbeta_dbin sampler: p[2, 1]
## [22] conjugate_dbeta_dbin sampler: p[2, 2]
## [23] conjugate_dbeta_dbin sampler: p[2, 3]
## [24] conjugate_dbeta_dbin sampler: p[2, 4]
## [25] conjugate_dbeta_dbin sampler: p[2, 5]
## [26] conjugate_dbeta_dbin sampler: p[2, 6]
## [27] conjugate_dbeta_dbin sampler: p[2, 7]
## [28] conjugate_dbeta_dbin sampler: p[2, 8]
## [29] conjugate_dbeta_dbin sampler: p[2, 9]
## [30] conjugate_dbeta_dbin sampler: p[2, 10]
## [31] conjugate_dbeta_dbin sampler: p[2, 11]
## [32] conjugate_dbeta_dbin sampler: p[2, 12]
## [33] conjugate_dbeta_dbin sampler: p[2, 13]
## [34] conjugate_dbeta_dbin sampler: p[2, 14]
## [35] conjugate_dbeta_dbin sampler: p[2, 15]
## [36] conjugate_dbeta_dbin sampler: p[2, 16]
```

# Customizing samplers: modifying the samplers


```r
hypers <- c('a[1]', 'b[1]', 'a[2]', 'b[2]')
for(h in hypers) {
      littersConf$removeSamplers(h)
      littersConf$addSampler(target = h, type = 'slice')
}
littersConf$printSamplers()
```

```
## [1]  conjugate_dbeta_dbin sampler: p[1, 1]
## [2]  conjugate_dbeta_dbin sampler: p[1, 2]
## [3]  conjugate_dbeta_dbin sampler: p[1, 3]
## [4]  conjugate_dbeta_dbin sampler: p[1, 4]
## [5]  conjugate_dbeta_dbin sampler: p[1, 5]
## [6]  conjugate_dbeta_dbin sampler: p[1, 6]
## [7]  conjugate_dbeta_dbin sampler: p[1, 7]
## [8]  conjugate_dbeta_dbin sampler: p[1, 8]
## [9]  conjugate_dbeta_dbin sampler: p[1, 9]
## [10] conjugate_dbeta_dbin sampler: p[1, 10]
## [11] conjugate_dbeta_dbin sampler: p[1, 11]
## [12] conjugate_dbeta_dbin sampler: p[1, 12]
## [13] conjugate_dbeta_dbin sampler: p[1, 13]
## [14] conjugate_dbeta_dbin sampler: p[1, 14]
## [15] conjugate_dbeta_dbin sampler: p[1, 15]
## [16] conjugate_dbeta_dbin sampler: p[1, 16]
## [17] conjugate_dbeta_dbin sampler: p[2, 1]
## [18] conjugate_dbeta_dbin sampler: p[2, 2]
## [19] conjugate_dbeta_dbin sampler: p[2, 3]
## [20] conjugate_dbeta_dbin sampler: p[2, 4]
## [21] conjugate_dbeta_dbin sampler: p[2, 5]
## [22] conjugate_dbeta_dbin sampler: p[2, 6]
## [23] conjugate_dbeta_dbin sampler: p[2, 7]
## [24] conjugate_dbeta_dbin sampler: p[2, 8]
## [25] conjugate_dbeta_dbin sampler: p[2, 9]
## [26] conjugate_dbeta_dbin sampler: p[2, 10]
## [27] conjugate_dbeta_dbin sampler: p[2, 11]
## [28] conjugate_dbeta_dbin sampler: p[2, 12]
## [29] conjugate_dbeta_dbin sampler: p[2, 13]
## [30] conjugate_dbeta_dbin sampler: p[2, 14]
## [31] conjugate_dbeta_dbin sampler: p[2, 15]
## [32] conjugate_dbeta_dbin sampler: p[2, 16]
## [33] slice sampler: a[1]
## [34] slice sampler: b[1]
## [35] slice sampler: a[2]
## [36] slice sampler: b[2]
```

```r
littersMCMC <- buildMCMC(littersConf)

## we need 'resetFunctions' because we are rebuilding the MCMC
## for an existing model for which we've already done some compilation

cLittersMCMC <- compileNimble(littersMCMC, project = littersModel,
                              resetFunctions = TRUE)
```

```
## compiling... this may take a minute. Use 'showCompilerOutput = TRUE' to see C++ compilation details.
```

```
## compilation finished.
```

```r
set.seed(1)
samplesSlice <- runMCMC(cLittersMCMC, niter = niter, nburnin = nburn,
             inits = littersInits, nchains = 1, samplesAsCodaMCMC = TRUE)
```

```
## running chain 1...
```

```
## |-------------|-------------|-------------|-------------|
## |-------------------------------------------------------|
```

# Customizing samplers: Initial results

We can look at diagnostics and see if the change in samplers had an effect. Interestingly, despite the posterior correlation between ```a[i]``` and ```b[i]```, a simple change just to the univariate samplers for the four hyperparameters has had some effect on MCMC performance.

Caveat: the real question is the effective sample size per unit of computation time (each slice sampler iteration is slower than each Metropolis iteration), but we don't assess that at the moment.



```r
library(coda, warn.conflicts = FALSE)
effectiveSize(samplesSlice)
```

```
##        a[1]        a[2]        b[1]        b[2]     p[1, 1]     p[2, 1] 
##    5.393745   32.451333    5.478835   34.484120   38.329445  527.251563 
##     p[1, 2]     p[2, 2]     p[1, 3]     p[2, 3]     p[1, 4]     p[2, 4] 
##   42.241548  411.503373   43.929929  749.000919   39.392606  728.382133 
##     p[1, 5]     p[2, 5]     p[1, 6]     p[2, 6]     p[1, 7]     p[2, 7] 
##   39.232571 2205.649855   35.940108 3058.268513   36.317496 2447.463984 
##     p[1, 8]     p[2, 8]     p[1, 9]     p[2, 9]    p[1, 10]    p[2, 10] 
##   40.235276 2966.353668   42.461277 2805.258729   38.610025 3728.995006 
##    p[1, 11]    p[2, 11]    p[1, 12]    p[2, 12]    p[1, 13]    p[2, 13] 
##   40.589594 4000.000000   41.259522 1603.461549   45.660256 1178.654140 
##    p[1, 14]    p[2, 14]    p[1, 15]    p[2, 15]    p[1, 16]    p[2, 16] 
##   33.822852 1313.530552   38.807405  377.661768   32.663616  216.465347
```

```r
library(basicMCMCplots)
basicMCMCplots::chainsPlot(samplesSlice,
                           var = c("a", "b"),
                           cex = 1.6)
```

![](figure/output-slice-1.png)

# Using JAGS

We'll briefly demonstrate using JAGS. A few things to keep in mind:

   - JAGS requires a text file with the BUGS code.
   - In general, we've found that JAGS choice of default samplers are well-chosen.
       - In some cases, that means that the default JAGS MCMC outperforms the default NIMBLE MCMC.
   - JAGS run-time computational speed sometimes beats NIMBLE's, particularly with conjugate samplers.


```r
library(rjags)
```

```
## Linked to JAGS 4.2.0
```

```
## Loaded modules: basemod,bugs
```

```r
cat("model {
  for (i in 1:G) {
     for (j in 1:N) {
        # likelihood (data model)
        r[i,j] ~ dbin(p[i,j], n[i,j])
        # latent process (random effects)
        p[i,j] ~ dbeta(a[i], b[i]) 
     }
     # prior for hyperparameters
     a[i] ~ dgamma(1, .001)
     b[i] ~ dgamma(1, .001)
   }
}", file = file.path(tempdir(), "tmp.bug"))

set.seed(2)  ## note: some other seeds result in slice sampler being stuck
inits <- littersInits
inits$p <- matrix(0.5, G, N)
```


```r
model <- jags.model(file = file.path(tempdir(), 'tmp.bug'),
      data = list(G = G, N = N, n = n, r = r),
      n.adapt = nburn, n.chains = 1, inits = inits)
      
samplesJags <- jags.samples(model, variable.names = c('a','b'), n.iter = niter)
```


```
## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 32
##    Unobserved stochastic nodes: 36
##    Total graph size: 110
## 
## Initializing model
## 
## 
## Deleting model
## 
## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 32
##    Unobserved stochastic nodes: 36
##    Total graph size: 110
## 
## Initializing model
```




```r
samplesJagsArray <- cbind(
    `a[1]` = samplesJags[[1]][1, , 1],
    `a[2]` = samplesJags[[1]][2, , 1],
    `b[1]` = samplesJags[[2]][1, , 1],
    `b[2]` = samplesJags[[2]][2, , 1]
)

basicMCMCplots::chainsPlot(samplesJagsArray,
                           cex = 1.6)
```

![](figure/jags-example4-1.png)

# Comparing results using chainsPlot

The `basicMCMCplots` library also let's us compare across chains



```r
samplesList <- list(
    nimble = samples,
    nimbleSlice = samplesSlice,
    JAGS = samplesJagsArray
)

basicMCMCplots::chainsPlot(samplesList,
                           var = c("a", "b"))
```

![](figure/unnamed-chunk-1-1.png)






















<!--
# Using Stan

We'll briefly demonstrate using Stan. A few things to keep in mind:

   - Stan uses Hamiltonian Monte Carlo, which is in general quite a bit slower per raw MCMC iteration but often more effective relative to the effective sample size.
        - Whether it is more effective relative to computational time likely depends on the problem. 
   - Stan's model declaration language is somewhat different from BUGS.
        - It's much more flexible than WinBUGS/JAGS and comparably flexible to NIMBLE.
-->




<!--
 Hmmm, that's not working but in a different fashion than for NIMBLE's default MCMC or for JAGS... We'll try again with Stan on a marginalized version of the model later.
-->


