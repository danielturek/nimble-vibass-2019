---
title: "Compiling parts of R using nimbleFunctions"
subtitle: "Valencia International Bayesian Analysis Summer School Workshop"
output: html_document
---




# Using *nimbleFunctions* to compile R code 

While nimbleFunctions are primarily designed to write algorithms to be applied to hierarchical models, you can also use nimbleFunctions as a way to compile your R code to fast C++ code without needing to write any C++ code. Note that this is unlike Rcpp, which provides tools for you to more easily write C++ code that can be called from R.

We've seen this use of nimbleFunctions already in our user-defined distributions. We'll explore this a bit more before seeing the additional details of how to interact with models.

# Uses of this functionality

How might you use this functionality?  Basically, this is useful for math-focused code that can't be easily vectorized in R. 

Caveats:

  - the NIMBLE compiler can't compile arbitrary R code, only code that is part of the NIMBLE *domain-specific language* (DSL)
  - you need to give a bit of information about types and dimensions of arguments (input) and return value (output)

# A basic demonstration

Consider the following, fully-vectorized calculation in R:


```r
out <- exp(cos(sin(x)) + x^3)
```

While this is pretty quick in R, it does suffer somewhat from how the R interpreter evaluates code. Here the R interpreter parses this code and executes the following operations in order, with temporary variables created along the way:

 - tmp1 <- sin(x)
 - tmp2 <- cos(tmp1)
 - tmp3 <- x^3
 - tmp4 <- tmp2 + tmp3
 - out <- exp(tmp4)

Here's a basic C-style implementation using a loop.


```r
nimMath1 <- nimbleFunction(
       run = function(x = double(1)) {
           returnType(double(1))
           n <- length(x)
           # some functions, like numeric, mimic R
           # but also may have additional/different features
           out <- numeric(n, init = FALSE)
           # core computation
           for( i in 1:n) 
                out[i] <- exp(cos(sin(x[i])) + x[i]^3)
           return(out)
})
cNimMath1 <- compileNimble(nimMath1)
```

```
## compiling... this may take a minute. Use 'showCompilerOutput = TRUE' to see C++ compilation details.
```

```
## compilation finished.
```

nimbleFunctions used in this way need a run function, type and dimension information for arguments and a *returnType()* line.

# Vectorization in nimbleFunctions

Actually nimbleFunctions can handle vectorized code using the Eigen linear algebra package, so let's consider a different implementation.


```r
nimMath2 <- nimbleFunction(
       run = function(x = double(1)) {
           returnType(double(1))
           out <- exp(cos(sin(x)) + x^3)
           return(out)
})
cNimMath2 <- compileNimble(nimMath2)
```

```
## compiling... this may take a minute. Use 'showCompilerOutput = TRUE' to see C++ compilation details.
```

```
## compilation finished.
```

For discussion: what does it mean to use vectorization in Eigen and does it suffer from the same use of temporary variables as discussed above in the R context?

# Bake-off


```r
x <- rnorm(1e6)
library(rbenchmark)
benchmark(out0 <- exp(cos(sin(x)) + x^3),
               out1 <- cNimMath1(x),
               out2 <- cNimMath2(x),
                columns = c('test','replications','elapsed'),
               replications = 10)
```

```
##                             test replications elapsed
## 1 out0 <- exp(cos(sin(x)) + x^3)           10   1.053
## 2           out1 <- cNimMath1(x)           10   0.972
## 3           out2 <- cNimMath2(x)           10   0.912
```

We could also time uncompiled execution of the nimbleFunction but it will generally be even slower than directly coding in R. The main purpose of running a nimbleFunction in R is debugging. You can step through execution of an uncompiled nimbleFunction using R's debugging tools as usual.

# A more involved example

Consider probit regression, which is similar to logistic regression. The probability of a binary outcome is given as
$p = P(Y = 1) = \Phi(X\beta)$ where $\Phi()$ is the normal CDF.

The probit model can be rewritten in a latent variable representation that in a Bayesian context can facilitate MCMC computations to fit the model:
$$ 
\begin{array}
Y & = &  I(W > 0) \\
W & \sim  & N(X\beta , 1) \\
\end{array}
$$

Suppose we know $\beta$. In order to determine $p$ we could use Monte Carlo simulation to estimate this integral:
$P(Y = 1) = \int_{0}^{\infty} f(w) dw$.

Now for probit regression, we could just use standard methods to compute normal pdf integrals (e.g., as implemented in `pnorm()`. But for the multinomial extension we discuss next, we need Monte Carlo simulation.

# Multinomial probit regression

Let $Y$ be a categorical variable, $Y \in \{{1,2,\ldots,K}\}$. Then a multinomial extension of the latent variable probit model is
$$
Y = {arg\ max}_k {W_k}
$$
$$
W_k \sim N(X\beta_k, 1)
$$

where the `arg max` is simply the $k$ that corresponds to the largest $W_k$.

Now to compute $p = ({P(Y=1), P(Y=2), \ldots, P(Y=K)})$ we can use Monte Carlo simulation. The basic steps are:

   - iterate m = 1, ... , M
      - for k = 1,...,K, sample $W_k$ from its corresponding normal distribution
      - determine the $k$ such that $W_k$ is the max
   - over the $M$ simulations, count the number of times each category had the largest corresponding $W_k$

The proportion of times each category had the  largest $W_k$ is an estimate of the multinomial proportions of interest.

For our example, we want to do this computation for large $M$ (to reduce Monte Carlo error). 

Note that in a real application, we would likely want to do this for multiple observations with an $n$ by $K$ matrix of $\alpha = X \beta$ values, resulting in an $n$ by $K$ matrix of proportions. But here we'll just consider a single $\alpha$.

# R implementation


```r
set.seed(1)
M <- 1000000
alphas <- c(-3, -0.5, -0.25, .1, .15, .29, .4, .45)  ## i.e., Xbeta from previous slide
K <- length(alphas)
system.time({
        # generate W_k ~ N(alpha_k, 1)
        rands <- matrix(rnorm(M*K), nrow = K, ncol = M)
        props <- rep(0, K)
        tmp <- alphas + rands # exploit vectorization
        # now tally the results
        id <- apply(tmp, 2, which.max)
        tbl <- table(id)
        props[as.integer(names(tbl))] <- tbl / M
        props
})
```

```
##    user  system elapsed 
##   2.973   0.175   3.234
```

# C-style implementation in NIMBLE


```r
mprobit <- nimbleFunction(
         run = function(alphas = double(1), M = double(0)) {
             returnType(double(1))
             K <- length(alphas)
             props <- numeric(K, value = 0)
             w <- numeric(K, init = FALSE)
             for(m in 1:M) {
                   for(k in 1:K) 
                        w[k] <- alphas[k] + rnorm(1) 
                   maxind <- 1
                   max <- w[1]
                   for(k in 2:K) {
                        if(w[k] > max){
                                maxind <- k
                                max <- w[k]          
                        }
                   }
                   props[maxind] <- props[maxind] + 1
             }
             props <- props/M
             return(props)
         }
)
```

# (Partially) vectorized implementation in NIMBLE


```r
mprobitVec <- nimbleFunction(
         run = function(alphas = double(1), M = double(0)) {
             returnType(double(1))
             K <- length(alphas)
             props <- numeric(K, value = 0)
             for(m in 1:M) {
                   w <- alphas + rnorm(K)
                   mx = max(w)
                   maxind = which(w == mx)                   
                   props[maxind] <- props[maxind] + 1
             }
             props <- props/M
             return(props)
         }
)
```

# Let's compare


```r
cmprobit = compileNimble(mprobit)
```

```
## compiling... this may take a minute. Use 'showCompilerOutput = TRUE' to see C++ compilation details.
```

```
## compilation finished.
```

```r
cmprobitVec = compileNimble(mprobitVec)
```

```
## compiling... this may take a minute. Use 'showCompilerOutput = TRUE' to see C++ compilation details.
## compilation finished.
```

```r
set.seed(1)
system.time(
props2 <- cmprobit(alphas, M)
)
```

```
##    user  system elapsed 
##   0.392   0.004   0.402
```

```r
system.time(
props3 <- cmprobitVec(alphas, M)
)
```

```
##    user  system elapsed 
##   1.092   0.012   1.116
```

Any speculation as to why the vectorized implementation is slower?

So we get a nice six-fold speedup, even though all of the R code was vectorized. In the exercises, you can practice with an example of replacing an explicit for loop in R.

# Using nimbleFunctions in BUGS code (user-defined functions)

You can write a nimbleFunction like the above examples and use them directly in your BUGS code to define a model.

Examples in my own work:

 - writing custom functions to vectorize calculations
 - writing a function to limit (restict) calculations to a local regons, only
 - writing a function to sum over discrete latent states

For example, see <a href="https://link.springer.com/article/10.1007/s10651-016-0353-z" target="_blank" style="color: blue">Turek et. al. (2016)</a>.
