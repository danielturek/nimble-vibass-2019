---
title: "Customizing an MCMC: extended example"
subtitle: "Valencia International Bayesian Analysis Summer School Workshop"
output: html_document
---



# The litters model

Here's the graph of the litters model.

<center><img src="littersDAG.jpg"></center>

Here we set up the litters model.


```r
library(nimble)
littersCode <- nimbleCode({
  for (i in 1:G) {
     for (j in 1:N) {
        # likelihood (data model)
        r[i,j] ~ dbin(p[i,j], n[i,j])
        # latent process (random effects)
        p[i,j] ~ dbeta(a[i], b[i]) 
     }
     # prior for hyperparameters
     a[i] ~ dgamma(1, .001)
     b[i] ~ dgamma(1, .001)
   }
})
```

```r
## data and constants as R objects
G <- 2
N <- 16
n <- matrix(c(13, 12, 12, 11, 9, 10, 
              9, 9, 8, 11, 8, 10, 13, 10, 12, 9, 10, 9, 10, 5, 9, 9, 13, 
              7, 5, 10, 7, 6, 10, 10, 10, 7), nrow = 2)
r <- matrix(c(13, 12, 12, 11, 9, 10, 9, 9, 8, 10, 8, 9, 
     12, 9, 11, 8, 9, 8, 9, 4, 8, 7, 11, 4, 4, 5, 5, 3, 7, 3, 7, 0), 
     nrow = 2)
              
littersConsts <- list(G = G, N = N, n = n)
littersData <- list(r = r)
littersInits <- list( a = c(2, 2), b=c(2, 2) )

## create the NIMBLE model object
littersModel <- nimbleModel(littersCode, 
          data = littersData, constants = littersConsts, inits = littersInits)
```

```
## defining model...
```

```
## building model...
```

```
## setting data and initial values...
```

```
## running calculate on model (any error reports that follow may simply reflect missing values in model variables) ... 
## checking model sizes and dimensions... This model is not fully initialized. This is not an error. To see which variables are not initialized, use model$initializeInfo(). For more information on model initialization, see help(modelInitialization).
## model building finished.
```

```r
cLittersModel <- compileNimble(littersModel)
```

```
## compiling... this may take a minute. Use 'showCompilerOutput = TRUE' to see C++ compilation details.
```

```
## compilation finished.
```


# Blocking parameters

Often a key factor that reduces MCMC performance is dependence between parameters that limits the ability of univariate samplers to move very far. A standard strategy is to sample correlated parameters in blocks. Unlike many other MCMC engines, NIMBLE makes it easy for users to choose what parameters to sample in blocks.

We'll try that here for ```a``` and ```b```.




```r
niter <- 5000
nburn <- 1000

littersConf <- configureMCMC(littersModel, monitors = c('a', 'b', 'p'))
hypers <- littersModel$getNodeNames(topOnly = TRUE)
print(hypers)
```

```
## [1] "a[1]" "a[2]" "b[1]" "b[2]"
```

```r
for(h in hypers) {
      littersConf$removeSamplers(h)
}
littersConf$addSampler(target = c('a[1]','b[1]'), type = 'RW_block', 
                              control = list(adaptInterval = 100))
```

```
## Note: Assigning an RW_block sampler to nodes with very different scales can result in low MCMC efficiency.  If all nodes assigned to RW_block are not on a similar scale, we recommend providing an informed value for the "propCov" control list argument, or using the AFSS sampler instead.
```

```r
littersConf$addSampler(target = c('a[2]','b[2]'), type = 'RW_block', 
                              control = list(adaptInterval = 100))
```

```
## Note: Assigning an RW_block sampler to nodes with very different scales can result in low MCMC efficiency.  If all nodes assigned to RW_block are not on a similar scale, we recommend providing an informed value for the "propCov" control list argument, or using the AFSS sampler instead.
```

```r
littersMCMC <- buildMCMC(littersConf)
cLittersMCMC <- compileNimble(littersMCMC, project = littersModel, resetFunctions = TRUE)
```

```
## compiling... this may take a minute. Use 'showCompilerOutput = TRUE' to see C++ compilation details.
```

```
## compilation finished.
```

```r
set.seed(1)
samplesBlock <- runMCMC(cLittersMCMC, niter = niter, nburnin = nburn,
             inits = littersInits, nchains = 1, samplesAsCodaMCMC = TRUE)
```

```
## running chain 1...
```

```
## |-------------|-------------|-------------|-------------|
## |-------------------------------------------------------|
```


```r
effectiveSize(samplesBlock)
```

```
##        a[1]        a[2]        b[1]        b[2]     p[1, 1]     p[2, 1] 
##   27.362933  115.432859   24.862000  144.562089    7.351846  955.264702 
##     p[1, 2]     p[2, 2]     p[1, 3]     p[2, 3]     p[1, 4]     p[2, 4] 
##    6.329953  977.760767    7.692498 1121.390015    7.496067 1088.444244 
##     p[1, 5]     p[2, 5]     p[1, 6]     p[2, 6]     p[1, 7]     p[2, 7] 
##    7.816839 3158.716920    9.348141 3272.955636    7.262044 3146.271066 
##     p[1, 8]     p[2, 8]     p[1, 9]     p[2, 9]    p[1, 10]    p[2, 10] 
##    8.427064 2600.492004    8.476167 2327.603143    8.334127 3313.877077 
##    p[1, 11]    p[2, 11]    p[1, 12]    p[2, 12]    p[1, 13]    p[2, 13] 
##    8.924759 2894.666705    6.158094 1418.018884    9.461952 1810.173081 
##    p[1, 14]    p[2, 14]    p[1, 15]    p[2, 15]    p[1, 16]    p[2, 16] 
##    7.716687 1082.088154    8.757271  844.456806    9.210670  285.871741
```

```r
makePlot(samplesBlock)
```

![](figure/output-block-1.png)

The block sampler seems to help some, but hopefully we can do better. Often block sampling gives bigger improvements.


# Blocking the random effects too

But perhaps we should have blocked the hyperparameters with their dependent random effects. This is how one could do that, though ```a```, ```b```, and ```p``` are on very different scales, which may cause problems, particularly at the start of an adaptive sampler. As we see in the trace plots, this strategy is not working at all.


```r
littersConf$removeSamplers(c('a', 'b', 'p'))
group1nodes <- littersModel$getDependencies(c('a[1]', 'b[1]'), stochOnly = TRUE)
group2nodes <- littersModel$getDependencies(c('a[2]', 'b[2]'), stochOnly = TRUE)
group1nodes
```

```
##  [1] "a[1]"     "b[1]"     "p[1, 1]"  "p[1, 2]"  "p[1, 3]"  "p[1, 4]" 
##  [7] "p[1, 5]"  "p[1, 6]"  "p[1, 7]"  "p[1, 8]"  "p[1, 9]"  "p[1, 10]"
## [13] "p[1, 11]" "p[1, 12]" "p[1, 13]" "p[1, 14]" "p[1, 15]" "p[1, 16]"
```

```r
propCov <- diag(c(.5, .5, rep(.01, 16)))
littersConf$addSampler(group1nodes, 'RW_block', control =
                       list(adaptInterval = 100, propCov = propCov))
```

```
## Note: Assigning an RW_block sampler to nodes with very different scales can result in low MCMC efficiency.  If all nodes assigned to RW_block are not on a similar scale, we recommend providing an informed value for the "propCov" control list argument, or using the AFSS sampler instead.
```

```r
littersConf$addSampler(group2nodes, 'RW_block', control =
                       list(adaptInterval = 100, propCov = propCov))
```

```
## Note: Assigning an RW_block sampler to nodes with very different scales can result in low MCMC efficiency.  If all nodes assigned to RW_block are not on a similar scale, we recommend providing an informed value for the "propCov" control list argument, or using the AFSS sampler instead.
```

```r
littersMCMC <- buildMCMC(littersConf)
cLittersMCMC <- compileNimble(littersMCMC, project = littersModel, resetFunctions = TRUE)
```

```
## compiling... this may take a minute. Use 'showCompilerOutput = TRUE' to see C++ compilation details.
```

```
## compilation finished.
```

```r
set.seed(1)
samplesSuperblock <- runMCMC(cLittersMCMC, niter = niter, nburnin = nburn,
                  inits = littersInits, nchains = 1, samplesAsCodaMCMC = TRUE)
```

```
## running chain 1...
```

```
## |-------------|-------------|-------------|-------------|
## |-------------------------------------------------------|
```


```r
effectiveSize(samplesSuperblock)
```

```
##      a[1]      a[2]      b[1]      b[2]   p[1, 1]   p[2, 1]   p[1, 2] 
##  6.438452 21.615065 28.042433 74.645083 48.534354 28.006915 20.787493 
##   p[2, 2]   p[1, 3]   p[2, 3]   p[1, 4]   p[2, 4]   p[1, 5]   p[2, 5] 
## 36.781123 14.701000 62.111234 10.357441 19.258878 20.213169 22.870547 
##   p[1, 6]   p[2, 6]   p[1, 7]   p[2, 7]   p[1, 8]   p[2, 8]   p[1, 9] 
## 34.713531 22.397978 15.005403 14.715280 16.143224 13.588992 16.274640 
##   p[2, 9]  p[1, 10]  p[2, 10]  p[1, 11]  p[2, 11]  p[1, 12]  p[2, 12] 
## 15.474444 37.851551 10.619684 30.206760 29.564242 28.882173  7.910076 
##  p[1, 13]  p[2, 13]  p[1, 14]  p[2, 14]  p[1, 15]  p[2, 15]  p[1, 16] 
## 22.380320  4.891401 28.946002 13.124093 15.571711 20.111385  7.755626 
##  p[2, 16] 
## 27.225063
```

```r
makePlot(samplesSuperblock)
```

![](figure/output-superblock-1.png)

I suspect that more time for adaptation is needed (and perhaps better initialization of the proposal covariance).

# Implicitly integrating over the random effects: cross-level sampler

Note that in this model, one could analytically integrate over the random effects (necessarily so since we have conjugacy). In NIMBLE this is pretty easy to do using user-defined distributions (next module), though it requires some technical knowledge of working with distributions.

An easier alternative to analytically integrating over the random effects is to use a computational trick that mathematically achieves the same result.

That is NIMBLE's *cross-level sampler*. Here's what it does:

  - do a blocked Metropolis random walk on one or more hyperparameters and
  - then a conjugate update of the dependent nodes conditional on the proposed hyperparameters,
  - accepting/rejecting everything together

Comments: 
  - this amounts to a joint update of the hyperparameters and their dependent nodes
  - equivalent to analytically integrating over the dependent nodes
  - this is the *one-block* sampler in <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9469.00308" target="_blank" style="color: blue">Knorr-Held and Rue (2002)</a>

# Applying the cross-level sampler



```r
littersConf$removeSamplers(c('a', 'b', 'p'))
littersConf$addSampler(c('a[1]', 'b[1]'), 'crossLevel')
littersConf$addSampler(c('a[2]', 'b[2]'), 'crossLevel')

littersMCMC <- buildMCMC(littersConf)
cLittersMCMC <- compileNimble(littersMCMC, project = littersModel, resetFunctions = TRUE)
```

```
## compiling... this may take a minute. Use 'showCompilerOutput = TRUE' to see C++ compilation details.
```

```
## compilation finished.
```

```r
set.seed(1)
samplesCross <- runMCMC(cLittersMCMC, niter = niter, nburnin = nburn,
             inits = littersInits, nchains = 1, samplesAsCodaMCMC = TRUE)
```

```
## running chain 1...
```

```
## |-------------|-------------|-------------|-------------|
## |-------------------------------------------------------|
```

# Cross-level sampler results


```r
effectiveSize(samplesCross)
```

```
##     a[1]     a[2]     b[1]     b[2]  p[1, 1]  p[2, 1]  p[1, 2]  p[2, 2] 
## 174.7002 123.7721 164.2023 161.8536 384.3497 501.6920 406.9321 675.6185 
##  p[1, 3]  p[2, 3]  p[1, 4]  p[2, 4]  p[1, 5]  p[2, 5]  p[1, 6]  p[2, 6] 
## 389.3645 526.6298 404.1256 610.6861 326.2830 597.5895 351.2667 663.2615 
##  p[1, 7]  p[2, 7]  p[1, 8]  p[2, 8]  p[1, 9]  p[2, 9] p[1, 10] p[2, 10] 
## 312.7498 726.8757 361.5656 502.1509 352.7005 675.2713 400.2829 620.3657 
## p[1, 11] p[2, 11] p[1, 12] p[2, 12] p[1, 13] p[2, 13] p[1, 14] p[2, 14] 
## 349.5235 723.0496 356.7016 603.1882 353.4078 593.5425 330.0875 656.0906 
## p[1, 15] p[2, 15] p[1, 16] p[2, 16] 
## 360.1016 622.1808 376.3246 332.3953
```

```r
makePlot(samplesCross)
```

![](figure/output-cross-level-1.png)

Much better, though we'd still want to look into the lack of movement for `a[1], b[1]` in the initial non-burnin samples -- this could probably be improved with better initialization of the top-level block sampler. 

